{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Colorization\n",
    "### Deep learning final project for conversion of gray scale images to rgb\n",
    "### Contributors: Bhumi Bhanushali, Avinash Hemaeshwara Raju, Kathan Nilesh Mehta, Atulay Ravishankar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget -N images.cocodataset.org/zips/train2017.zip\n",
    "# wget -N images.cocodataset.org/zips/val2017.zip\n",
    "# wget -N images.cocodataset.org/zips/test2017.zip\n",
    "# pip3 install tensorboard\n",
    "# tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from network_definition import Colorization\n",
    "from skimage import io, color\n",
    "from skimage.transform import resize\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    model_file_name = 'checkpoint.pt'\n",
    "    load_model_to_train = False\n",
    "    load_model_to_test = False\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    point_batches = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    epochs = 1\n",
    "    batch_size = 1\n",
    "    learning_rate = 0.001\n",
    "    num_workers = 8\n",
    "    learning_rate_decay = 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "config = Configuration()\n",
    "hparams = HyperParameters()\n",
    "print('Device:',config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, process_type):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [f for f in os.listdir(root_dir)]\n",
    "        self.process_type = process_type\n",
    "        print(self.process_type,'[0]\\t',self.root_dir,self.files[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.process_type == 'train':\n",
    "            return 100#len(self.files)\n",
    "        else:\n",
    "            return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.rgb = io.imread(os.path.join(self.root_dir,self.files[index]))\n",
    "        rgb_encoder = resize(self.rgb, (224, 224),anti_aliasing=True)\n",
    "        rgb_inception = resize(self.rgb, (300, 300),anti_aliasing=True)\n",
    "\n",
    "        self.lab_encoder = color.rgb2lab(rgb_encoder)\n",
    "        l_encoder = self.lab_encoder[:,:,0]\n",
    "        l_encoder = np.stack((l_encoder,)*3,axis = -1)\n",
    "        l_encoder = torchvision.transforms.ToTensor()(l_encoder)\n",
    "        ab_encoder = self.lab_encoder[:,:,1:3]\n",
    "        ab_encoder = torchvision.transforms.ToTensor()(ab_encoder)\n",
    "\n",
    "        self.lab_inception = color.rgb2lab(rgb_inception)\n",
    "        l_inception = self.lab_inception[:,:,0]\n",
    "        l_inception = np.stack((l_inception,)*3,axis = -1)\n",
    "        l_inception = torchvision.transforms.ToTensor()(l_inception)\n",
    "\n",
    "        return l_encoder, ab_encoder, l_inception, torchvision.transforms.ToTensor()(rgb_encoder)\n",
    "\n",
    "    def show_rgb(self, index):\n",
    "        self.__getitem__(index)\n",
    "        print(\"RGB image size:\", self.rgb.shape)\n",
    "        cv2.imshow(\"RGB\",self.rgb)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    def show_lab_encoder(self, index):\n",
    "        self.__getitem__(index)\n",
    "        print(\"Encoder Lab image size:\", self.lab_encoder.shape)\n",
    "        cv2.imshow(\"Lab Encoder\",self.lab_encoder)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    def show_lab_inception(self, index):\n",
    "        self.__getitem__(index)\n",
    "        print(\"Inception Lab image size:\", self.lab_inception.shape)\n",
    "        cv2.imshow(\"Lab Inception\",self.lab_encoder)\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.model = self.model.float()\n",
    "        return self.model(x.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FusionLayer,self).__init__()\n",
    "\n",
    "    def forward(self, inputs, mask=None):\n",
    "        ip, emb = inputs\n",
    "        emb = torch.stack([torch.stack([emb],dim=2)],dim=3)\n",
    "        emb = emb.repeat(1,1,ip.shape[2],ip.shape[3])\n",
    "        fusion = torch.cat((ip,emb),1)\n",
    "        return fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_depth):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_depth, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2.0),\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2.0),\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor=2.0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorization(nn.Module):\n",
    "    def __init__(self, depth_after_fusion):\n",
    "        super(Colorization,self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.fusion = FusionLayer()\n",
    "        self.after_fusion = nn.Conv2d(in_channels=1256, out_channels=depth_after_fusion,kernel_size=1, stride=1,padding=0)\n",
    "        self.decoder = Decoder(depth_after_fusion)\n",
    "\n",
    "    def forward(self, img_l, img_emb):\n",
    "        img_enc = self.encoder(img_l)\n",
    "        fusion = self.fusion([img_enc, img_emb])\n",
    "        fusion = self.after_fusion(fusion)\n",
    "        return self.decoder(fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Colorization(256).to(config.device) \n",
    "inception_model = models.inception_v3(pretrained=True).float().to(config.device)\n",
    "inception_model = inception_model.float()\n",
    "inception_model.eval()\n",
    "loss_criterion = torch.nn.MSELoss(reduction='mean').to(config.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=hparams.learning_rate, weight_decay=1e-6)\n",
    "milestone_list  = list(range(0,hparams.epochs,2))\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestone_list, gamma=hparams.learning_rate_decay)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [0]\t data/train 000000109622.jpg\n",
      "validation [0]\t data/validation 000000182611.jpg\n",
      "test [0]\t data/test 000000220208.jpg\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset('data/train','train')\n",
    "validataion_dataset = CustomDataset('data/validation','validation')\n",
    "test_dataset = CustomDataset('data/test','test')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams.batch_size, shuffle=True, num_workers=hparams.num_workers)\n",
    "validation_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams.batch_size, shuffle=False, num_workers=hparams.num_workers)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=hparams.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 1\n",
      "Batch: 0 | Processing time for 10 : 2.805928945541382 | Batch Loss: 33.74257202148438\n",
      "Batch: 10 | Processing time for 10 : 17.890161752700806 | Batch Loss: 305.5652221679687\n",
      "Batch: 20 | Processing time for 10 : 18.239235162734985 | Batch Loss: 217.08550338745118\n",
      "Batch: 30 | Processing time for 10 : 18.205628871917725 | Batch Loss: 187.57070410251617\n",
      "Batch: 40 | Processing time for 10 : 17.732488870620728 | Batch Loss: 135.05602016448975\n",
      "Batch: 50 | Processing time for 10 : 17.673827171325684 | Batch Loss: 228.61508407592774\n",
      "Batch: 60 | Processing time for 10 : 17.780488967895508 | Batch Loss: 213.291748046875\n",
      "Batch: 70 | Processing time for 10 : 17.75102710723877 | Batch Loss: 145.469234085083\n",
      "Batch: 80 | Processing time for 10 : 17.676505088806152 | Batch Loss: 197.42254810333253\n",
      "Batch: 90 | Processing time for 10 : 17.57918405532837 | Batch Loss: 278.00376052856444\n",
      "Training Loss: 208.44994093179702 | Processed in  179.03973412513733 s\n",
      "Validation Loss: 208.23980222940446 | Processed in  52.7022979259491 s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-98e2cfc5fda5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m     checkpoint = {'model': model,'model_state_dict': model.state_dict(),\\\n\u001b[1;32m     77\u001b[0m                   \u001b[0;34m'optimizer'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                   \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                   'val_loss':val_loss, 'val_acc':val_acc}\n\u001b[1;32m     80\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_acc' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(hparams.epochs):\n",
    "    print('Starting epoch:',epoch+1)\n",
    "\n",
    "    #*** Training step ***\n",
    "    loop_start = time.time()\n",
    "    avg_loss = 0.0\n",
    "    batch_loss = 0.0\n",
    "    main_start = time.time()\n",
    "    \n",
    "    for idx,(img_l_encoder, img_ab_encoder, img_l_inception, img_rgb) in enumerate(train_dataloader):\n",
    "        #*** Move data to GPU if available ***\n",
    "        img_l_encoder = img_l_encoder.to(config.device)\n",
    "        img_ab_encoder = img_ab_encoder.to(config.device)\n",
    "        img_l_inception = img_l_inception.to(config.device)\n",
    "\n",
    "        #*** Initialize model & Optimizer ***\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #*** Forward Propagation ***\n",
    "        img_embs = inception_model(img_l_inception.float())\n",
    "        output_ab = model(img_l_encoder,img_embs)\n",
    "\n",
    "        #*** Back propogation ***\n",
    "        loss = loss_criterion(output_ab, img_ab_encoder.float())\n",
    "        loss.backward()\n",
    "\n",
    "        #*** Weight Update ****\n",
    "        optimizer.step()\n",
    "\n",
    "        #*** Reduce Learning Rate ***\n",
    "        scheduler.step()\n",
    "\n",
    "        #*** Loss Calculation ***\n",
    "        avg_loss += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "        #*** Print stats after every point_batches ***\n",
    "        if idx%config.point_batches==0: \n",
    "            loop_end = time.time()   \n",
    "            print('Batch:',idx, '| Processing time for',config.point_batches,':',loop_end-loop_start,\\\n",
    "                  '| Batch Loss:', batch_loss/config.point_batches)\n",
    "            loop_start = time.time()\n",
    "            batch_loss = 0.0\n",
    "\n",
    "    #*** Print Training Data Stats ***\n",
    "    train_loss = avg_loss/len(train_dataloader)*hparams.batch_size\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    print('Training Loss:',train_loss,'| Processed in ',time.time()-main_start,'s')\n",
    "\n",
    "    #*** Validation Step ***       \n",
    "    avg_loss = 0.0\n",
    "    loop_start = time.time()\n",
    "    for idx,(img_l_encoder, img_ab_encoder, img_l_inception, img_rgb) in enumerate(validation_dataloader):\n",
    "        #*** Move data to GPU if available ***\n",
    "        img_l_encoder = img_l_encoder.to(config.device)\n",
    "        img_ab_encoder = img_ab_encoder.to(config.device)\n",
    "        img_l_inception = img_l_inception.to(config.device)\n",
    "\n",
    "        #*** Intialize Model to Eval Mode ***\n",
    "        model.eval()\n",
    "\n",
    "        #*** Forward Propagation ***\n",
    "        img_embs = inception_model(img_l_inception.float())\n",
    "        output_ab = model(img_l_encoder,img_embs)\n",
    "\n",
    "        #*** Loss Calculation ***\n",
    "        loss = loss_criterion(output_ab, img_ab_encoder.float())\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "    val_loss = avg_loss/len(validation_dataloader)*hparams.batch_size\n",
    "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "    print('Validation Loss:', val_loss,'| Processed in ',time.time()-loop_start,'s')\n",
    "\n",
    "    #*** Save the Model to disk ***\n",
    "    checkpoint = {'model': model,'model_state_dict': model.state_dict(),\\\n",
    "                  'optimizer' : optimizer,'optimizer_state_dict' : optimizer.state_dict(),\\\n",
    "                  'train_loss':train_loss, 'val_loss':val_loss}\n",
    "    torch.save(checkpoint, config.model_file_name)\n",
    "    print(\"Model saved at:\",os.getcwd(),'/',config.model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert Tensor Image -> Numpy Image -> Color  Image -> Tensor Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorization(\n",
      "  (encoder): Encoder(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (fusion): FusionLayer()\n",
      "  (after_fusion): Conv2d(1256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (decoder): Decoder(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (8): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): Tanh()\n",
      "      (12): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatente_and_colorize(im_lab, img_ab):\n",
    "    # Assumption is that im_lab is of size [1,3,224,224]\n",
    "    print(im_lab.size(),img_ab.size())\n",
    "    np_img = im_lab[0].detach().numpy().transpose(1,2,0)\n",
    "    lab = np.empty([*np_img.shape[0:2], 3])\n",
    "    lab[:, :, 0] = np.squeeze(((np_img + 1) * 50))\n",
    "    lab[:, :, 1:] = img_ab[0].detach().numpy().transpose(1,2,0) * 127\n",
    "    np_img = color.lab2rgb(lab)\n",
    "    color_im = torch.stack([torchvision.transforms.ToTensor()(np_img)],dim=0)\n",
    "    return color_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(im_lab):\n",
    "    # Assumption is that im_lab is of size [1,3,224,224]\n",
    "    np_img = im_lab[0].detach().numpy().transpose(1,2,0)\n",
    "    np_img = color.lab2rgb(np_img)\n",
    "    color_im = torch.stack([torchvision.transforms.ToTensor()(np_img)],dim=0)\n",
    "    return color_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/skimage/color/colorconv.py:1068: UserWarning: Color data out of range: Z < 0 in 392 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/skimage/color/colorconv.py:1068: UserWarning: Color data out of range: Z < 0 in 391 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/skimage/color/colorconv.py:1068: UserWarning: Color data out of range: Z < 0 in 112 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/skimage/color/colorconv.py:1068: UserWarning: Color data out of range: Z < 0 in 2140 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/skimage/color/colorconv.py:1068: UserWarning: Color data out of range: Z < 0 in 193 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/skimage/color/colorconv.py:1068: UserWarning: Color data out of range: Z < 0 in 3810 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/skimage/color/colorconv.py:1068: UserWarning: Color data out of range: Z < 0 in 6503 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/skimage/color/colorconv.py:1068: UserWarning: Color data out of range: Z < 0 in 4296 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n",
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 2, 224, 224])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-5c9a440f2b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#*** Forward Propagation ***\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimg_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minception_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_l_inception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moutput_ab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_l_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# N x 768 x 17 x 17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# N x 768 x 17 x 17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#*** Inference Step ***\n",
    "avg_loss = 0.0\n",
    "loop_start = time.time()\n",
    "for idx,(img_l_encoder, img_ab_encoder, img_l_inception, img_rgb) in enumerate(test_dataloader):\n",
    "        #*** Move data to GPU if available ***\n",
    "        img_l_encoder = img_l_encoder.to(config.device)\n",
    "        img_ab_encoder = img_ab_encoder.to(config.device)\n",
    "        img_l_inception = img_l_inception.to(config.device)\n",
    "        \n",
    "        #*** Intialize Model to Eval Mode ***\n",
    "        model.eval()\n",
    "        \n",
    "        #*** Forward Propagation ***\n",
    "        img_embs = inception_model(img_l_inception.float())\n",
    "        output_ab = model(img_l_encoder,img_embs)\n",
    "        \n",
    "        #*** Adding l channel to ab channels ***\n",
    "        img_lab = concatente_and_colorize(torch.stack([img_l_encoder[:,0,:,:]],dim=1),output_ab)\n",
    "#         img_lab = torch.cat((torch.stack([img_l_encoder[:,0,:,:]],dim=1).float(),output_ab),1)\n",
    "#        img_lab = colorize(img_lab)\n",
    "        \n",
    "        #*** Printing to Tensor Board ***\n",
    "        grid = torchvision.utils.make_grid(img_lab)\n",
    "        writer.add_image('Output Lab Images', grid, 0)\n",
    "        \n",
    "        #*** Loss Calculation ***\n",
    "        loss = loss_criterion(output_ab, img_ab_encoder.float())\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "test_loss = avg_loss/len(test_dataloader)\n",
    "writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "print('Test Loss:',avg_loss/len(test_dataloader),'| Processed in ',time.time()-loop_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
