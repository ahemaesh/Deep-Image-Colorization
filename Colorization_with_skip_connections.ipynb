{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHjIH1IseNpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wget -N images.cocodataset.org/zips/train2017.zip\n",
        "# wget -N images.cocodataset.org/zips/val2017.zip\n",
        "# wget -N images.cocodataset.org/zips/test2017.zip\n",
        "# pip3 install tensorboard\n",
        "# tensorboard --logdir=runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmPX99WweNpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOoNvXY7eNp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Configuration:\n",
        "    model_file_name = 'checkpoint.pt'\n",
        "    load_model_to_train = True\n",
        "    load_model_to_test = True\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    point_batches = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGKXUNhdeNp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HyperParameters:\n",
        "    epochs = 30\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.001\n",
        "    num_workers = 16\n",
        "    learning_rate_decay = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3H-g8iseNp_",
        "colab_type": "code",
        "outputId": "5a579737-fbd3-4499-dd3a-a96765245894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "config = Configuration()\n",
        "hparams = HyperParameters()\n",
        "print('Device:',config.device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9u9OMCzeNqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, process_type):\n",
        "        self.root_dir = root_dir\n",
        "        self.files = [f for f in os.listdir(root_dir)]\n",
        "        self.process_type = process_type\n",
        "        print('File[0]:',self.files[0],'| Total Files:', len(self.files), '| Process:',self.process_type,)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            #*** Read the image from file ***\n",
        "            self.rgb_img = cv2.imread(os.path.join(self.root_dir,self.files[index])).astype(np.float32) \n",
        "            self.rgb_img /= 255.0 \n",
        "            \n",
        "            #*** Resize the color image to pass to encoder ***\n",
        "            rgb_encoder_img = cv2.resize(self.rgb_img, (224, 224))\n",
        "            \n",
        "            #*** Resize the color image to pass to decoder ***\n",
        "            rgb_inception_img = cv2.resize(self.rgb_img, (300, 300))\n",
        "            \n",
        "            ''' Encoder Images '''\n",
        "            #*** Convert the encoder color image to normalized lab space ***\n",
        "            self.lab_encoder_img = cv2.cvtColor(rgb_encoder_img,cv2.COLOR_BGR2Lab) \n",
        "            \n",
        "            #*** Splitting the lab images into l-channel, a-channel, b-channel ***\n",
        "            l_encoder_img, a_encoder_img, b_encoder_img = self.lab_encoder_img[:,:,0],self.lab_encoder_img[:,:,1],self.lab_encoder_img[:,:,2]\n",
        "            \n",
        "            #*** Normalizing l-channel between [-1,1] ***\n",
        "            l_encoder_img = l_encoder_img/50.0 - 1.0\n",
        "            \n",
        "            #*** Repeat the l-channel to 3 dimensions ***\n",
        "            l_encoder_img = torchvision.transforms.ToTensor()(l_encoder_img)\n",
        "            l_encoder_img = l_encoder_img.expand(3,-1,-1)\n",
        "            \n",
        "            #*** Normalize a and b channels and concatenate ***\n",
        "            a_encoder_img = (a_encoder_img/128.0)\n",
        "            b_encoder_img = (b_encoder_img/128.0)\n",
        "            a_encoder_img = torch.stack([torch.Tensor(a_encoder_img)])\n",
        "            b_encoder_img = torch.stack([torch.Tensor(b_encoder_img)])\n",
        "            ab_encoder_img = torch.cat([a_encoder_img, b_encoder_img], dim=0)\n",
        "            \n",
        "            ''' Inception Images '''\n",
        "            #*** Convert the inception color image to lab space ***\n",
        "            self.lab_inception_img = cv2.cvtColor(rgb_inception_img,cv2.COLOR_BGR2Lab)\n",
        "            \n",
        "            #*** Extract the l-channel of inception lab image *** \n",
        "            l_inception_img = self.lab_inception_img[:,:,0]/50.0 - 1.0\n",
        "             \n",
        "            #*** Convert the inception l-image to torch Tensor and stack it in 3 channels ***\n",
        "            l_inception_img = torchvision.transforms.ToTensor()(l_inception_img)\n",
        "            l_inception_img = l_inception_img.expand(3,-1,-1)\n",
        "            \n",
        "            ''' return images to data-loader '''\n",
        "            rgb_encoder_img = torchvision.transforms.ToTensor()(rgb_encoder_img)\n",
        "            return l_encoder_img, ab_encoder_img, l_inception_img, rgb_encoder_img, self.files[index]\n",
        "        \n",
        "        except Exception as e:\n",
        "            print('Exception at ',self.files[index], e)\n",
        "            return torch.tensor(-1), torch.tensor(-1), torch.tensor(-1), torch.tensor(-1), 'Error'\n",
        "\n",
        "    def show_rgb(self, index):\n",
        "        self.__getitem__(index)\n",
        "        print(\"RGB image size:\", self.rgb_img.shape)        \n",
        "        cv2.imshow(self.rgb_img)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    def show_lab_encoder(self, index):\n",
        "        self.__getitem__(index)\n",
        "        print(\"Encoder Lab image size:\", self.lab_encoder_img.shape)\n",
        "        cv2.imshow(self.lab_encoder_img)\n",
        "        c2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    def show_lab_inception(self, index):\n",
        "        self.__getitem__(index)\n",
        "        print(\"Inception Lab image size:\", self.lab_inception_img.shape)\n",
        "        cv2.imshow(self.lab_inception_img)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "    \n",
        "    def show_other_images(self, index):\n",
        "        a,b,c,d,_ = self.__getitem__(index)\n",
        "        print(\"Encoder l channel image size:\",a.shape)\n",
        "        cv2.imshow((a.detach().numpy().transpose(1,2,0)))\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "        print(\"Encoder ab channel image size:\",b.shape)\n",
        "        cv2.imshow((b.detach().numpy().transpose(1,2,0)[:,:,0]))\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "        cv2.imshow((b.detach().numpy().transpose(1,2,0)[:,:,1]))\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "        print(\"Inception l channel image size:\",c.shape)\n",
        "        cv2.imshow(c.detach().numpy().transpose(1,2,0))\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "        print(\"Color resized image size:\",d.shape)\n",
        "        cv2.imshow(d.detach().numpy().transpose(1,2,0))\n",
        "        cv2.waitKey(0) \n",
        "        cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeA07bqveNqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = CustomDataset('/content/train2017','train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxvHYPKpeNqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset.show_rgb(0)\n",
        "train_dataset.show_lab_encoder(0)\n",
        "train_dataset.show_lab_inception(0)\n",
        "train_dataset.show_other_images(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6IVH3vlaKpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class my_model(nn.Module):\n",
        "    def __init__(self, depth_after_fusion):\n",
        "        super(my_model,self).__init__()\n",
        "        \n",
        "        # Encoder Layer Network\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1) \n",
        "        self.r1 = nn.ReLU(inplace=True)\n",
        "        self.b1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.r2 = nn.ReLU(inplace=True)\n",
        "        self.b2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
        "        self.r3 = nn.ReLU(inplace=True)\n",
        "        self.b3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.r4 = nn.ReLU(inplace=True)\n",
        "        self.b4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
        "        self.r5 = nn.ReLU(inplace=True)\n",
        "        self.b5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.r6 = nn.ReLU(inplace=True)\n",
        "        self.b6 = nn.BatchNorm2d(512)\n",
        "        self.conv7 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.r7 = nn.ReLU(inplace=True)\n",
        "        self.b7 = nn.BatchNorm2d(512)\n",
        "        self.conv8 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.r8 = nn.ReLU(inplace=True)\n",
        "        self.b8 = nn.BatchNorm2d(256)\n",
        "\n",
        "        # Fusion Layer\n",
        "        self.after_fusion = nn.Conv2d(in_channels=1256, out_channels=depth_after_fusion,kernel_size=1, stride=1,padding=0)\n",
        "        self.after_fusion_res = nn.Conv2d(in_channels=depth_after_fusion*2, out_channels=depth_after_fusion,kernel_size=1, stride=1,padding=0)\n",
        "        self.bf = nn.BatchNorm2d(256)\n",
        "        self.rf = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Decoder Layer Network\n",
        "        self.conv9 = nn.Conv2d(in_channels=depth_after_fusion, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.r9 = nn.ReLU(inplace=True)\n",
        "        self.b9 = nn.BatchNorm2d(128)\n",
        "        self.u9 = nn.Upsample(scale_factor=2.0)\n",
        "        self.conv10 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.r10 = nn.ReLU(inplace=True)\n",
        "        self.b10 = nn.BatchNorm2d(64)\n",
        "        self.conv11 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.r11 = nn.ReLU(inplace=True)\n",
        "        self.b11 = nn.BatchNorm2d(64)\n",
        "        self.u11 = nn.Upsample(scale_factor=2.0)\n",
        "        self.conv12 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.r12 = nn.ReLU(inplace=True)\n",
        "        self.b12 = nn.BatchNorm2d(32)\n",
        "        self.conv13 = nn.Conv2d(in_channels=32, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
        "        self.t13 = nn.Tanh()\n",
        "        self.u13 = nn.Upsample(scale_factor=2.0)\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        #self.model = self.model.float()\n",
        "        \n",
        "        # Encoder forward\n",
        "        y = self.conv1(x)\n",
        "        skip_y1 = self.b1(y)\n",
        "        y = self.r1(skip_y1)\n",
        "        \n",
        "        y = self.conv2(y)\n",
        "        y = self.b2(y)\n",
        "        y = self.r2(y)\n",
        "        \n",
        "        y = self.conv3(y)\n",
        "        skip_y2 = self.b3(y)\n",
        "        y = self.r3(skip_y2)\n",
        "        \n",
        "        y = self.conv4(y)\n",
        "        y = self.b4(y)\n",
        "        y = self.r4(y)\n",
        "        \n",
        "        y = self.conv5(y)\n",
        "        y = self.b5(y)\n",
        "        y = self.r5(y)\n",
        "        \n",
        "        y = self.conv6(y)\n",
        "        y = self.b6(y)\n",
        "        y = self.r6(y)\n",
        "        \n",
        "        y = self.conv7(y)\n",
        "        y = self.b7(y)\n",
        "        y = self.r7(y)\n",
        "        \n",
        "        y = self.conv8(y)\n",
        "        skip_y3 = self.b8(y)\n",
        "        y = self.r8(skip_y3)\n",
        "        \n",
        "        # Fusion layer\n",
        "        emb = torch.stack([torch.stack([emb],dim=2)],dim=3)\n",
        "        emb = emb.repeat(1,1,y.shape[2],y.shape[3])\n",
        "        fusion = torch.cat((y,emb),1)\n",
        "        y = self.after_fusion(fusion)\n",
        "        y = torch.cat((y, skip_y3), 1) # Skip connection\n",
        "\n",
        "        y = self.after_fusion_res(y)\n",
        "        y = self.bf(y)\n",
        "        y = self.rf(y)\n",
        "\n",
        "        # Decoder forward\n",
        "        y = self.u9(y)\n",
        "        y = self.conv9(y)\n",
        "        y = torch.cat((y, skip_y2), 1) # Skip connection\n",
        "        \n",
        "        y = self.conv9(y)\n",
        "        y = self.b9(y)\n",
        "        y = self.r9(y)\n",
        "          \n",
        "        y = self.u11(y)\n",
        "        y = self.conv10(y)\n",
        "        y = torch.cat((y, skip_y1), 1) # Skip connection\n",
        "\n",
        "        y = self.conv10(y)\n",
        "        y = self.b10(y)\n",
        "        y = self.r10(y)\n",
        "        \n",
        "        y = self.conv11(y)\n",
        "        y = self.b11(y)\n",
        "        y = self.r11(y)\n",
        "\n",
        "        y = self.conv12(y)\n",
        "        y = self.b12(y)\n",
        "        y = self.r12(y)\n",
        "        \n",
        "        y = self.conv13(y)\n",
        "        y = self.t13(y)\n",
        "        y = self.u13(y)\n",
        "              \n",
        "        return y.float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNl57kDUeNqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if config.load_model_to_train or config.load_model_to_test:\n",
        "    checkpoint = torch.load(\"/content/drive/My Drive/IDL Project/Models/Coco_checkpoint10.pt\",map_location=torch.device(config.device))\n",
        "    model = checkpoint['model']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(config.device) \n",
        "    optimizer = checkpoint['optimizer']\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    print('Loaded pretrain model | Previous train loss:',checkpoint['train_loss'])\n",
        "else:\n",
        "    model = my_model(256).to(config.device) \n",
        "    # model.apply(init_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=hparams.learning_rate, weight_decay=1e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4rrNBp_1bup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTbF_-fsyDZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inception_model = models.inception_v3(pretrained=True).float().to(config.device)\n",
        "inception_model = inception_model.float()\n",
        "inception_model.eval()\n",
        "loss_criterion = torch.nn.MSELoss(reduction='mean').to(config.device)\n",
        "milestone_list  = list(range(0,hparams.epochs,2))\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EbW1TPzeNqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not config.load_model_to_test:\n",
        "    train_dataset = CustomDataset('/content/train2017','train')\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams.batch_size, shuffle=True, num_workers=hparams.num_workers)\n",
        "    \n",
        "\n",
        "    # validataion_dataset = CustomDataset('data/validation','validation')\n",
        "    # validation_dataloader = torch.utils.data.DataLoader(validataion_dataset, batch_size=hparams.batch_size, shuffle=False, num_workers=hparams.num_workers)\n",
        "    \n",
        "    print('Train:',len(train_dataloader), '| Total Images:',len(train_dataloader)*hparams.batch_size)\n",
        "    # print('Valid:',len(validation_dataloader), '| Total Images:',len(validation_dataloader)*hparams.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "M3LWlA9eeNqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not config.load_model_to_test:\n",
        "    for epoch in range(0, hparams.epochs):\n",
        "        print('Starting epoch:',epoch+1)\n",
        "\n",
        "        #*** Training step ***\n",
        "        loop_start = time.time()\n",
        "        avg_loss = 0.0\n",
        "        batch_loss = 0.0\n",
        "        main_start = time.time()\n",
        "        model.train()\n",
        "\n",
        "        for idx,(img_l_encoder, img_ab_encoder, img_l_inception, img_rgb, file_name) in enumerate(train_dataloader):\n",
        "            #*** Skip bad data ***\n",
        "            if not img_l_encoder.ndim:\n",
        "                continue\n",
        "\n",
        "            #*** Move data to GPU if available ***\n",
        "            img_l_encoder = img_l_encoder.to(config.device)\n",
        "            img_ab_encoder = img_ab_encoder.to(config.device)\n",
        "            img_l_inception = img_l_inception.to(config.device)\n",
        "\n",
        "            #*** Initialize Optimizer ***\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #*** Forward Propagation ***\n",
        "            img_embs = inception_model(img_l_inception.float())\n",
        "            output_ab = model(img_l_encoder,img_embs)\n",
        "\n",
        "            #*** Back propogation ***\n",
        "            loss = loss_criterion(output_ab, img_ab_encoder.float())\n",
        "            loss.backward()\n",
        "\n",
        "            #*** Weight Update ****\n",
        "            optimizer.step()\n",
        "\n",
        "            #*** Loss Calculation ***\n",
        "            avg_loss += loss.item()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "            #*** Print stats after every point_batches ***\n",
        "            if idx%config.point_batches==0: \n",
        "                loop_end = time.time()   \n",
        "                print('Batch:',idx, '| Processing time for',config.point_batches,':',loop_end-loop_start,'s | Batch Loss:', batch_loss/config.point_batches)\n",
        "                loop_start = time.time()\n",
        "                batch_loss = 0.0\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        #*** Print Training Data Stats ***\n",
        "        train_loss = avg_loss/len(train_dataloader)*hparams.batch_size\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        print('Training Loss:',train_loss,'| Processed in ',time.time()-main_start,'s')\n",
        "\n",
        "        #*** Reduce Learning Rate ***\n",
        "        scheduler.step(train_loss)\n",
        "\n",
        "        # #*** Validation Step ***       \n",
        "        # avg_loss = 0.0\n",
        "        # loop_start = time.time()\n",
        "        # #*** Intialize Model to Eval Mode for validation ***\n",
        "        # model.eval()\n",
        "        # for idx,(img_l_encoder, img_ab_encoder, img_l_inception, img_rgb, file_name) in enumerate(validation_dataloader):\n",
        "        #     #*** Skip bad data ***\n",
        "        #     if not img_l_encoder.ndim:\n",
        "        #         continue\n",
        "\n",
        "        #     #*** Move data to GPU if available ***\n",
        "        #     img_l_encoder = img_l_encoder.to(config.device)\n",
        "        #     img_ab_encoder = img_ab_encoder.to(config.device)\n",
        "        #     img_l_inception = img_l_inception.to(config.device)\n",
        "\n",
        "        #     #*** Forward Propagation ***\n",
        "        #     img_embs = inception_model(img_l_inception.float())\n",
        "        #     output_ab = model(img_l_encoder,img_embs)\n",
        "\n",
        "        #     #*** Loss Calculation ***\n",
        "        #     loss = loss_criterion(output_ab, img_ab_encoder.float())\n",
        "        #     avg_loss += loss.item()\n",
        "\n",
        "        # val_loss = avg_loss/len(validation_dataloader)*hparams.batch_size\n",
        "        # writer.add_scalar('Loss/validation', val_loss, epoch)\n",
        "        # print('Validation Loss:', val_loss,'| Processed in ',time.time()-loop_start,'s')\n",
        "\n",
        "        # #*** Save the Model to disk ***\n",
        "        checkpoint = {'model': model,'model_state_dict': model.state_dict(), 'optimizer' : optimizer,'optimizer_state_dict' : optimizer.state_dict(), 'train_loss':train_loss}\n",
        "        torch.save(checkpoint, \"/content/drive/My Drive/IDL Project/Models/Coco_checkpoint\"+str(epoch)+\".pt\")\n",
        "        print(\"Model saved at:\",os.getcwd(),'/',config.model_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUgFS8SJeNqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = CustomDataset('/content/drive/My Drive/val2017','test')\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "print('Test: ',len(test_dataloader), '| Total Image:',len(test_dataloader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsMWt1y5eNq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concatente_and_colorize(im_lab, img_ab):\n",
        "    # Assumption is that im_lab is of size [1,3,224,224]\n",
        "    #print(im_lab.size(),img_ab.size())\n",
        "    np_img = im_lab[0].cpu().detach().numpy().transpose(1,2,0)\n",
        "    lab = np.empty([*np_img.shape[0:2], 3],dtype=np.float32)\n",
        "    lab[:, :, 0] = np.squeeze(((np_img + 1) * 50))\n",
        "    lab[:, :, 1:] = img_ab[0].cpu().detach().numpy().transpose(1,2,0) * 127\n",
        "    np_img = cv2.cvtColor(lab,cv2.COLOR_Lab2RGB) \n",
        "    color_im = torch.stack([torchvision.transforms.ToTensor()(np_img)],dim=0)\n",
        "    return color_im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwu8A9eteNq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_color(l_img, ab_img):\n",
        "    np_img = l_img[0].detach().numpy().transpose(1,2,0)\n",
        "    plt.imshow(np_img[:,:,0])\n",
        "    ab_img = ab_img[0].detach().numpy().transpose(1,2,0)\n",
        "    print(np.min(ab_img[:,:,0]), np.max(ab_img[:,:,0]))\n",
        "    plt.imshow(ab_img[:,:,0])\n",
        "    print(np.min(ab_img[:,:,1]), np.max(ab_img[:,:,1]))\n",
        "    plt.imshow(ab_img[:,:,1])\n",
        "    print(np_img.shape,ab_img.shape)\n",
        "    np_img = np.concatenate((np_img,ab_img),axis=2)\n",
        "    color_np_img = cv2.cvtColor(np_img,cv2.COLOR_Lab2RGB) \n",
        "    return color_np_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH6ZaLsueNrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def colorize(im_lab):\n",
        "    # Assumption is that im_lab is of size [1,3,224,224]\n",
        "    np_img = im_lab[0].detach().numpy().transpose(1,2,0)\n",
        "    np_img = color.lab2rgb(np_img)\n",
        "    color_im = torch.stack([torchvision.transforms.ToTensor()(np_img)],dim=0)\n",
        "    return color_im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_J3UR6UeNrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#*** Inference Step ***\n",
        "avg_loss = 0.0\n",
        "loop_start = time.time()\n",
        "for idx,(img_l_encoder, img_ab_encoder, img_l_inception, img_rgb, file_name) in enumerate(test_dataloader):\n",
        "        #*** Skip bad data ***\n",
        "        if not img_l_encoder.ndim:\n",
        "            continue\n",
        "            \n",
        "        #*** Move data to GPU if available ***\n",
        "        img_l_encoder = img_l_encoder.to(config.device)\n",
        "        img_ab_encoder = img_ab_encoder.to(config.device)\n",
        "        img_l_inception = img_l_inception.to(config.device)\n",
        "        \n",
        "        #*** Intialize Model to Eval Mode ***\n",
        "        model.eval()\n",
        "        \n",
        "        #*** Forward Propagation ***\n",
        "        img_embs = inception_model(img_l_inception.float())\n",
        "        print(torch.min(img_l_encoder),torch.max(img_l_encoder))\n",
        "        print(torch.min(img_embs),torch.max(img_embs))\n",
        "        output_ab = model(img_l_encoder,img_embs)\n",
        "        \n",
        "        #*** Adding l channel to ab channels ***\n",
        "        color_img = concatente_and_colorize(torch.stack([img_l_encoder[:,0,:,:]],dim=1),output_ab)\n",
        "        #img_lab = concatente_and_colorize(torch.stack([img_l_encoder[:,0,:,:]],dim=1),output_ab)\n",
        "        color_img_jpg = color_img[0].detach().numpy().transpose(1,2,0)\n",
        "        # plt.imshow(color_img_jpg)\n",
        "        # plt.show()\n",
        "        # plt.imsave('outputs/'+file_name[0],color_img_jpg)\n",
        "        save_image(color_img[0], '/content/drive/My Drive/Output_after_epoch12_coco/' + file_name[0]) \n",
        "        \n",
        "        \n",
        "        \n",
        "#         #*** Printing to Tensor Board ***\n",
        "#         grid = torchvision.utils.make_grid(img_lab)\n",
        "#         writer.add_image('Output Lab Images', grid, 0)\n",
        "        \n",
        "        #*** Loss Calculation ***\n",
        "        loss = loss_criterion(output_ab, img_ab_encoder.float())\n",
        "        avg_loss += loss.item()\n",
        "        \n",
        "test_loss = avg_loss/len(test_dataloader)\n",
        "writer.add_scalar('Loss/test', test_loss, epoch)\n",
        "print('Test Loss:',avg_loss/len(test_dataloader),'| Processed in ',time.time()-loop_start,'s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGGR_22QeNrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}